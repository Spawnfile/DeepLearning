{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F # for activation functions. On the other hand, for adding act. func. we can use torch.nn as well.\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1)) # values has been chosen arbitrarily.\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "        self.fc1 = nn.Linear(16*7*7, num_classes) # 16 => output channel of conv2 layer, 7*7 means after two pooling layer with 2x2 kernel, 28x28 input will be decrease to 7x7.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def save_checkpoint(state, filename=\"./my_checkpoint.pt\"):\n",
    "    print(\"Saving Checkpoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint):\n",
    "    print(\"Loading Checkpoint\")\n",
    "    print(checkpoint.keys())\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "\n",
    "\n",
    "model = CNN()\n",
    "x = torch.rand(64,1,28,28)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device setup\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "in_channel = 1\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True) # ToTensor() converts original data that loaded from dataset library as np.array, to tensor. \n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True) # Shuffle makes us sure about do not have same images on batches on different epochs.\n",
    "\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT MODEl\n",
    "model = CNN().to(device) # send to GPU if you have CUDA.  \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "writer = SummaryWriter(f\"runs/CIFAR10_tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading Checkpoint\n",
      "dict_keys(['state_dict', 'optimizer'])\n",
      "Saving Checkpoint\n",
      "Loss at epoch 0 was 0.00335\n",
      "Loss at epoch 1 was 0.00195\n",
      "Loss at epoch 2 was 0.00259\n",
      "Saving Checkpoint\n",
      "Loss at epoch 3 was 0.00156\n",
      "Loss at epoch 4 was 0.00491\n",
      "Loss at epoch 5 was 0.00142\n",
      "Saving Checkpoint\n",
      "Loss at epoch 6 was 0.00030\n",
      "Loss at epoch 7 was 0.00324\n",
      "Loss at epoch 8 was 0.00474\n",
      "Saving Checkpoint\n",
      "Loss at epoch 9 was 0.00072\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "if load_model:\n",
    "    load_checkpoint(torch.load('my_checkpoint.pt'))\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    if epoch % 3 == 0:\n",
    "        checkpoint = {'state_dict': model.state_dict(), 'optimizer':optimizer.state_dict()} # state dict is a python object that stores model's ( it can store optimizers parameters as well) parameters like conv1-bias, weight; conv2-bias weight and fc-bias weights etc.\n",
    "        save_checkpoint(checkpoint)\n",
    "\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader): # data = img tensor, targets = labels tensor in related batch.\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        # forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        _, predictions = scores.max(1)\n",
    "        num_correct = (predictions == targets).sum()\n",
    "        running_train_acc = float(num_correct) / float(data.shape[0])\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad() # set gradient putput to zero for re-calculate gradient in every loop. We DO NOT want get interacted by previous gradient value.\n",
    "        loss.backward() # compute gradient for every parameter\n",
    "\n",
    "        # gradient descent or adam step \n",
    "        optimizer.step() # performs parameter update based on current computed gradient.\n",
    "        writer.add_scalar(\"Trainin Loss :\", loss, global_step=step)\n",
    "        writer.add_scalar(\"Trainin ACC :\", running_train_acc, global_step=step)\n",
    "        step += 1\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses) # avarage of losses list.\n",
    "    print(f\"Loss at epoch {epoch} was {mean_loss:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Checking acc on training data\n",
      "Got59639 / 60000 with acc, 99.40\n",
      "Checking acc on test data\n",
      "Got9870 / 10000 with acc, 98.70\n"
     ]
    }
   ],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print(\"Checking acc on training data\")\n",
    "    else:\n",
    "        print(\"Checking acc on test data\")\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval() # switch to evaluating ( inference ) mode\n",
    "\n",
    "    with torch.no_grad(): # this context manager guarantees us that we are NOT CALCULATING gradient during this process.\n",
    "        for x,y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            scores = model(x)  # scores output shape = 64x10\n",
    "            value, predictions = scores.max(1) # we take max value of second dimension for getting which number have been predicted by NN.\n",
    "            num_correct += (predictions == y).sum()  \n",
    "            num_samples += predictions.size(0)   \n",
    "        print(f\"Got{num_correct} / {num_samples} with acc, {float(num_correct)/float(num_samples)*100:.2f}\") # 95.78 etc.\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}